# Colab: end-to-end AI Audit pipeline (pages_content_audit_2)
# NOTE: Replace all placeholders (DB creds, GCP project/location, ScaNN function names, Vertex API calls)
# before running in production. This script is designed to be run interactively in Colab.

# 1) Installs (uncomment in Colab if needed)
# !pip install --upgrade google-cloud-aiplatform psycopg2-binary sqlalchemy pandas numpy scipy rapidfuzz

import os
import json
import time
import math
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple
from scipy.special import softmax

# ------------- USER CONFIG: REPLACE BEFORE RUNNING -------------
DB_HOST = os.getenv("ALLOYDB_HOST", "<ALLOYDB_HOST>")   # e.g., "34.123.45.67"
DB_PORT = int(os.getenv("ALLOYDB_PORT", "5432"))
DB_USER = os.getenv("ALLOYDB_USER", "postgres")
DB_PASSWORD = os.getenv("ALLOYDB_PASSWORD", "<DB_PASSWORD>")
DB_NAME = os.getenv("ALLOYDB_NAME", "mydb")

GCP_PROJECT = os.getenv("GCP_PROJECT", "<YOUR_GCP_PROJECT_ID>")
GCP_LOCATION = os.getenv("GCP_LOCATION", "us-central1")

# Vertex / embedding model names (placeholders)
EMBEDDING_MODEL = "gemini-text-embedding-001"   # replace with actual Vertex embedding model name
PARAPHRASE_MODEL = "text-bison@001"            # replace if using Vertex text generation
CROSS_ENCODER_MODEL = "cross-encoder-domain-model"  # your cross-encoder model (placeholder)

# Thresholds & parameters (tune on validation)
T_VEC_INIT = 0.30        # vector similarity threshold for ScaNN (cosine)
T_BM25_INIT = 1.5        # BM25 ts_rank threshold
T_FUSE = 0.60            # fused score threshold for NLI gating
NLI_KEEP = 0.80
NLI_CONTRADICT = 0.60

# Progressive expansion safety
S_SAFE = 2000
S_HARD = 10000
MIN_KEEP = 10
MAX_PROCESS = 5000  # internal processing cap to avoid unbounded compute

# ------------- DB Utilities -------------
import psycopg2
from psycopg2.extras import Json

def connect_db():
    conn = psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        dbname=DB_NAME
    )
    conn.autocommit = True
    return conn

def safe_json_load(s):
    if s is None:
        return {}
    try:
        return json.loads(s) if isinstance(s, str) else s
    except Exception:
        return {}

# ------------- SQL: table creation & populate -------------
CREATE_TABLE_SQL = """
CREATE TABLE IF NOT EXISTS pages_content_audit_2 (
  id BIGINT PRIMARY KEY,
  original_site_id TEXT,
  page_url TEXT,
  meta_data JSONB,
  search_keywords TSVECTOR,
  page_content_html TEXT,
  embedding_text TEXT,
  content_embedding FLOAT8[],
  embedding_model TEXT,
  quality_score FLOAT8 DEFAULT 0.0,
  indexed_at TIMESTAMP DEFAULT now()
);
"""

POPULATE_SQL = """
-- Populate pages_content_audit_2 from site_pages_data. Keeps existing rows untouched.
INSERT INTO pages_content_audit_2 (id, original_site_id, page_url, meta_data, page_content_html, embedding_text)
SELECT id,
       original_site_id,
       page_url,
       COALESCE(meta_data, '{}'::jsonb),
       page_content_html,
       COALESCE(
         nullif(trim(both from page_content_html), ''),
         (COALESCE((meta_data->>'title'), '') || ' ' || COALESCE((meta_data->>'description'), '')),
         ''
       ) AS embedding_text
FROM site_pages_data
ON CONFLICT (id) DO NOTHING;
"""

INDEX_SQL = """
-- GIN index for full-text search
CREATE INDEX IF NOT EXISTS idx_pages_search_keywords ON pages_content_audit_2 USING GIN (search_keywords);

-- ScaNN index placeholder for AlloyDB - replace with your DB's ScaNN syntax if available.
-- Example (Pseudo): CREATE INDEX IF NOT EXISTS idx_pages_scann ON pages_content_audit_2 USING scann (content_embedding);
"""

# ------------- Embedding ingestion placeholder -------------
# In Colab, replace generate_embedding_demo with a Vertex embeddings call (no API key: use GCP auth)
def generate_embedding_demo(text: str):
    """Deterministic demo embedding for local testing. Replace with real Vertex embedding call."""
    if not text or text.strip() == '':
        return []
    rng = np.random.RandomState(abs(hash(text)) % (2**32))
    v = rng.randn(768).astype(float)
    norm = np.linalg.norm(v)
    if norm > 0:
        v = v / norm
    return v.tolist()

def ingest_embeddings(conn, batch_size=128, dry_run=False):
    """Find rows with empty content_embedding and populate embeddings (demo)."""
    with conn.cursor() as cur:
        cur.execute("SELECT id, embedding_text FROM pages_content_audit_2 WHERE content_embedding IS NULL OR array_length(content_embedding,1) = 0")
        rows = cur.fetchall()
        print('Rows to embed:', len(rows))
        for i in range(0, len(rows), batch_size):
            batch = rows[i:i+batch_size]
            updates = []
            for _id, text in batch:
                if not text or text.strip() == '':
                    emb = []
                else:
                    emb = generate_embedding_demo(text)
                updates.append((_id, emb))
            if not dry_run:
                for _id, emb in updates:
                    cur.execute("UPDATE pages_content_audit_2 SET content_embedding = %s, embedding_model = %s, indexed_at = now() WHERE id = %s",
                                (emb, EMBEDDING_MODEL, _id))
            print(f'Batched embedding update: {i}..{i+len(batch)}')

# ------------- DSPy-like paraphrase generation (placeholder) -------------
def generate_paraphrases_demo(query_text: str, n: int = 4) -> List[str]:
    qs = []
    qs.append(query_text.strip())
    qs.append(query_text.strip() + " (refined)")
    qs.append("Find pages about: " + query_text.strip())
    qs.append(query_text.strip().rstrip('?') + " - search")
    return qs[:n]

# ------------- BM25 & ScaNN SQL templates (placeholders) -------------
BM25_SQL_TEMPLATE = """
SELECT id, page_url, meta_data, page_content_html, ts_rank(search_keywords, plainto_tsquery(%s)) AS bm25_score
FROM pages_content_audit_2
WHERE search_keywords @@ plainto_tsquery(%s)
  AND ts_rank(search_keywords, plainto_tsquery(%s)) >= %s;
"""

# NOTE: Replace 'scann_similarity' with your AlloyDB ScaNN function or use the SQL function provided by AlloyDB.
# If AlloyDB doesn't support radius search via SQL, you can request a large top-k and then filter by vec_sim >= T_VEC.
SCANN_SQL_TEMPLATE = """
-- PSEUDO-SQL: Replace scann_similarity with AlloyDB's function name or API
SELECT id, page_url, meta_data, page_content_html, scann_similarity(content_embedding, %s) AS vec_sim
FROM pages_content_audit_2
WHERE scann_similarity(content_embedding, %s) >= %s;
"""

# ------------- Normalization, fusion & utilities -------------
def normalize_and_fuse(df: pd.DataFrame, alpha=0.4, beta=0.1, gamma=0.05, temp=0.1) -> pd.DataFrame:
    if 'vec_sim' not in df.columns:
        df['vec_sim'] = 0.0
    if 'bm25_score' not in df.columns:
        df['bm25_score'] = 0.0
    df['vec_norm'] = (df['vec_sim'] + 1.0) / 2.0  # map cosine [-1,1] -> [0,1]
    bm_mean = df['bm25_score'].mean() if not df['bm25_score'].isnull().all() else 0.0
    bm_std  = df['bm25_score'].std(ddof=0) if df['bm25_score'].std(ddof=0) > 0 else 1.0
    df['bm_z'] = (df['bm25_score'] - bm_mean) / bm_std
    df['bm_norm'] = 1.0 / (1.0 + np.exp(-df['bm_z']))
    # optional boosts: compute from meta_data if present
    def compute_meta_boost(meta):
        try:
            m = safe_json_load(meta)
            # Example: boost by year recency or type
            year = int(m.get('year', 0)) if isinstance(m, dict) and m.get('year') else 0
            recency = 0.0
            if year > 2018:
                recency = min(0.2, (year - 2018) * 0.02)
            return recency
        except Exception:
            return 0.0
    df['meta_boost'] = df['meta_boost'] if 'meta_boost' in df.columns else df['meta_data'].apply(lambda x: compute_meta_boost(x))
    df['recency_boost'] = df.get('recency_boost', 0.0)
    df['fused'] = alpha * df['vec_norm'] + (1 - alpha) * df['bm_norm'] + beta * df['meta_boost'] + gamma * df['recency_boost']
    df['fused_softmax'] = softmax(df['fused'] / temp)
    return df

def elbow_cutoff(sorted_scores: List[float], min_keep=MIN_KEEP, max_keep=5000):
    s = np.array(sorted_scores)
    if len(s) <= min_keep:
        return len(s)
    diffs = np.diff(s)
    diffs2 = np.diff(diffs)
    if len(diffs2) == 0:
        return max(min_keep, len(s))
    elbow_idx = np.argmax(diffs2) + 2
    keep = max(min_keep, elbow_idx)
    return keep

# ------------- Cross-encoder NLI placeholder -------------
# Replace this with a real cross-encoder NLI model call (Vertex / local model). For demo we simulate probs.
def cross_encoder_nli_demo(pairs: List[Tuple[str, str]]) -> List[Dict[str, float]]:
    # returns list of dicts {entail: p1, neutral: p2, contradict: p3, rel_score: r}
    out = []
    for q, doc in pairs:
        p_entail = np.random.rand() * 0.8
        p_neutral = np.random.rand() * 0.3
        p_contradict = max(0.0, 1.0 - (p_entail + p_neutral))
        rel_score = np.random.rand()  # relative relevance score
        out.append({'entail': float(p_entail), 'neutral': float(p_neutral), 'contradict': float(p_contradict), 'rel_score': float(rel_score)})
    return out

# ------------- Progressive threshold expansion helper -------------
def progressive_union_and_limits(conn, q_text: str, q_vec: List[float], t_vec=T_VEC_INIT, t_bm25=T_BM25_INIT):
    """
    Retrieve candidates by threshold (no top-k) but progressively tighten thresholds if candidate explosion occurs.
    Returns a combined DataFrame of candidates.
    """
    # 1) BM25
    bm25_df = pd.DataFrame(columns=['id','page_url','meta_data','page_content_html','bm25_score'])
    try:
        with conn.cursor() as cur:
            cur.execute(BM25_SQL_TEMPLATE, (q_text, q_text, q_text, t_bm25))
            rows = cur.fetchall()
            if rows:
                bm25_df = pd.DataFrame(rows, columns=['id','page_url','meta_data','page_content_html','bm25_score'])
    except Exception as e:
        print('BM25 error (demo):', e)

    # 2) ScaNN threshold - placeholder; replace scann_similarity with your DB function as needed
    scann_df = pd.DataFrame(columns=['id','page_url','meta_data','page_content_html','vec_sim'])
    try:
        with conn.cursor() as cur:
            # If your DB doesn't have a scann_similarity function, request large top-k and filter by vec_sim >= t_vec.
            cur.execute(SCANN_SQL_TEMPLATE, (q_vec, q_vec, t_vec))
            rows = cur.fetchall()
            if rows:
                scann_df = pd.DataFrame(rows, columns=['id','page_url','meta_data','page_content_html','vec_sim'])
    except Exception as e:
        # placeholder failure handling (e.g., scann not available)
        # In demo mode, we'll generate an empty scann_df
        #print('ScaNN SQL call failed (placeholder):', e)
        scann_df = pd.DataFrame(columns=['id','page_url','meta_data','page_content_html','vec_sim'])

    # 3) Union & dedupe
    combined = pd.concat([bm25_df.set_index('id'), scann_df.set_index('id')], axis=1, sort=False).reset_index()
    if combined.shape[0] > 0:
        combined['meta_data'] = combined['meta_data'].apply(lambda x: safe_json_load(x))
        combined['vec_sim'] = combined.get('vec_sim', 0.0)
        combined['bm25_score'] = combined.get('bm25_score', 0.0)
    return combined

# ------------- Orchestrator: end-to-end query (demo) -------------
def end_to_end_query(conn, user_query: str, chosen_variant_index: int = None, show_snippets=True):
    """
    Orchestrate: paraphrase options -> user chooses a variant (simulated) -> retrieve by thresholds -> fuse -> NLI -> final results
    conn: psycopg2 connection or None for demo
    """
    print("User query:", user_query)
    paraphrases = generate_paraphrases_demo(user_query, n=4)
    print("Paraphrase suggestions:")
    for i, p in enumerate(paraphrases):
        print(f"{i+1}: {p}")
    # select variant (for demo use chosen_variant_index if given, else 1)
    if chosen_variant_index is None:
        chosen_variant_index = 0
    q_text = paraphrases[chosen_variant_index]

    # Get query embedding (placeholder/demo)
    q_vec = generate_embedding_demo(q_text)

    # Progressive retrieval (no top-k but bounded)
    combined = progressive_union_and_limits(conn, q_text, q_vec, t_vec=T_VEC_INIT, t_bm25=T_BM25_INIT)
    print("Initial candidate count (BM25 âˆª ScaNN thresholds):", len(combined))

    # If too many candidates, tighten thresholds progressively
    t_vec = T_VEC_INIT
    t_bm25 = T_BM25_INIT
    while len(combined) > S_SAFE and (t_vec < 0.95 or t_bm25 < 1e6):
        # tighten
        t_vec = min(0.95, t_vec + 0.05)
        t_bm25 = t_bm25 + 0.3
        print(f"Tightening thresholds: t_vec={t_vec:.3f}, t_bm25={t_bm25:.3f}")
        combined = progressive_union_and_limits(conn, q_text, q_vec, t_vec=t_vec, t_bm25=t_bm25)
        if t_vec >= 0.95 and t_bm25 > 1000:
            break
    if len(combined) > S_HARD:
        print("Candidate set still huge; streaming/early-exit will be required. Truncating at internal MAX_PROCESS for demo.")
        combined = combined.head(MAX_PROCESS)

    if combined.shape[0] == 0:
        print("No candidates found. Consider lowering thresholds.")
        return pd.DataFrame()

    # Normalize & fuse
    combined = normalize_and_fuse(combined)

    # Select by fused threshold (no top-k)
    nli_candidates = combined[combined['fused'] >= T_FUSE].copy()
    print("NLI candidate count (fused >= T_FUSE):", len(nli_candidates))

    # If none, relax T_FUSE progressively
    t_fuse = T_FUSE
    while nli_candidates.shape[0] == 0 and t_fuse > 0.1:
        t_fuse = max(0.1, t_fuse - 0.05)
        nli_candidates = combined[combined['fused'] >= t_fuse].copy()
        print("Relaxed T_FUSE to", t_fuse, "->", len(nli_candidates), "candidates")

    if nli_candidates.shape[0] == 0:
        print("No NLI candidates after relaxing. Returning top few fused candidates for inspection.")
        sample = combined.sort_values('fused', ascending=False).head(MIN_KEEP)
        return sample

    # Cross-encoder NLI (simulate for demo); produce (entail, neutral, contradict) and rel_score
    pairs = []
    for _, row in nli_candidates.iterrows():
        doc_text = (row['page_content_html'] or '')[:1000]
        pairs.append((q_text, doc_text))

    nli_results = cross_encoder_nli_demo(pairs)

    # Attach NLI predictions and decide
    decisions = []
    for idx, (i_row, res) in enumerate(zip(nli_candidates.index, nli_results)):
        p_entail, p_neutral, p_contradict = res['entail'], res['neutral'], res['contradict']
        if (p_entail + p_neutral) >= NLI_KEEP:
            decision = 'keep'
        elif p_contradict > NLI_CONTRADICT:
            decision = 'contradiction_demote'
        else:
            decision = 'uncertain_demote'
        nli_candidates.at[i_row, 'p_entail'] = p_entail
        nli_candidates.at[i_row, 'p_neutral'] = p_neutral
        nli_candidates.at[i_row, 'p_contradict'] = p_contradict
        nli_candidates.at[i_row, 'cross_rel_score'] = res['rel_score']
        nli_candidates.at[i_row, 'nli_decision'] = decision
        # demote scores if necessary
        final_score = nli_candidates.at[i_row, 'fused']
        if decision == 'contradiction_demote':
            final_score *= 0.2
        elif decision == 'uncertain_demote':
            final_score *= 0.6
        nli_candidates.at[i_row, 'final_score'] = final_score
        decisions.append(decision)

    # Final ranking by final_score desc
    final_df = nli_candidates.sort_values('final_score', ascending=False).reset_index(drop=True)
    print("Final results:", len(final_df))

    # Format display
    out_cols = ['id', 'page_url', 'meta_data', 'bm25_score', 'vec_sim', 'fused', 'p_entail', 'p_neutral', 'p_contradict', 'nli_decision', 'final_score']
    display_df = final_df[out_cols].copy()
    if show_snippets:
        display_df['snippet'] = final_df['page_content_html'].fillna('').str.slice(0, 400)
    # Pretty print top results for demo
    pd.set_option('display.max_colwidth', 200)
    if display_df.shape[0] > 0:
        print(display_df.head(50).to_string(index=False))
    return final_df

# ------------- Script usage instructions (print) -------------
if __name__ == '__main__':
    print("This script is a Colab-ready end-to-end demo.")
    print("Steps to run in Colab:")
    print("1) Replace DB & GCP placeholders at top of script.")
    print("2) Connect to AlloyDB via connect_db() and run the CREATE/POPULATE/INDEX SQL using psycopg2.")
    print("   Example:")
    print("     conn = connect_db()")
    print("     with conn.cursor() as cur: cur.execute(CREATE_TABLE_SQL); cur.execute(POPULATE_SQL); cur.execute(INDEX_SQL)")
    print("3) Ingest embeddings: ingest_embeddings(conn)  (replace generate_embedding_demo with Vertex embedding calls).")
    print("4) Call end_to_end_query(conn, 'your query') to run the demo retrieval pipeline.")
    print("")
    print("Important: Replace SCANN_SQL_TEMPLATE function name 'scann_similarity' with AlloyDB's actual function.")
