# full_colab_ai_audit.py
# End-to-end AI Audit pipeline (Colab-ready)
# Uses DB_URI instead of host/port. Replace placeholders (DB_URI, GCP values, model names) before using.

# 1) (Uncomment in Colab) Installs (if required)
# !pip install --upgrade google-cloud-aiplatform psycopg2-binary pandas numpy scipy

import os
import json
import time
import math
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple
from scipy.special import softmax
import psycopg2
from psycopg2.extras import Json

# ---------------------------
# USER CONFIG - REPLACE THESE
# ---------------------------
DB_URI = "postgresql://<USER>:<PASSWORD>@<HOST>:<PORT>/<DB_NAME>"  # <-- Paste your full AlloyDB URI here
DB_NAME = "<DB_NAME>"  # e.g., "postgres" (optional, used for messaging)
TABLE_SOURCE = "site_pages_data"      # existing table that has page_content_html etc
TABLE_TARGET = "pages_content_audit_2"  # the table this script will create/populate

# Vertex/Model placeholders - replace when you add real Vertex calls
GCP_PROJECT = "<YOUR_GCP_PROJECT_ID>"
GCP_LOCATION = "us-central1"
EMBEDDING_MODEL = "gemini-embedding-001"   # placeholder name
PARAPHRASE_MODEL = "text-bison@001"       # placeholder name
CROSS_ENCODER_MODEL = "cross-encoder-domain-model"  # placeholder

# Retrieval & NLI thresholds (tune with validation)
T_VEC_INIT = 0.30         # initial cosine threshold for ScaNN radius-like search
T_BM25_INIT = 1.5         # initial BM25 ts_rank threshold
T_FUSE = 0.60             # fused threshold to send docs to NLI
NLI_KEEP = 0.80
NLI_CONTRADICT = 0.60

# Progressive expansion caps (internal)
S_SAFE = 2000
S_HARD = 10000
MIN_KEEP = 10
MAX_PROCESS = 5000

# ---------------------------
# DB connection helpers
# ---------------------------
def connect_db():
    """Connect using a single DB URI (AlloyDB-style)."""
    conn = psycopg2.connect(DB_URI)
    conn.autocommit = True
    return conn

def safe_json_load(s):
    if s is None:
        return {}
    try:
        return json.loads(s) if isinstance(s, str) else s
    except Exception:
        return {}

# ---------------------------
# SQL: create and populate target table
# ---------------------------
CREATE_TABLE_SQL = f"""
CREATE TABLE IF NOT EXISTS {TABLE_TARGET} (
  id BIGINT PRIMARY KEY,
  original_site_id TEXT,
  page_url TEXT,
  meta_data JSONB,
  search_keywords TSVECTOR,
  page_content_html TEXT,
  embedding_text TEXT,
  content_embedding FLOAT8[],
  embedding_model TEXT,
  quality_score FLOAT8 DEFAULT 0.0,
  indexed_at TIMESTAMP DEFAULT now()
);
"""

POPULATE_SQL = f"""
-- Populate {TABLE_TARGET} from {TABLE_SOURCE}
INSERT INTO {TABLE_TARGET} (id, original_site_id, page_url, meta_data, page_content_html, embedding_text)
SELECT id,
       COALESCE(original_site_id, '') AS original_site_id,
       COALESCE(page_url, '') AS page_url,
       COALESCE(meta_data, '{{}}'::jsonb) AS meta_data,
       COALESCE(page_content_html, '') AS page_content_html,
       COALESCE(
         nullif(trim(both from page_content_html), ''),
         (COALESCE((meta_data->>'title'), '') || ' ' || COALESCE((meta_data->>'description'), '')),
         ''
       ) AS embedding_text
FROM {TABLE_SOURCE}
ON CONFLICT (id) DO NOTHING;
"""

# Create search_keywords (tsvector) by combining embedding_text + title/desc
CREATE_SEARCH_KEYWORDS_SQL = f"""
-- Populate search_keywords tsvector from embedding_text + meta_data.title + meta_data.description
UPDATE {TABLE_TARGET}
SET search_keywords = to_tsvector('english',
    COALESCE(embedding_text,'') || ' ' ||
    COALESCE((meta_data->>'title'), '') || ' ' ||
    COALESCE((meta_data->>'description'), '')
)
WHERE (search_keywords IS NULL);
"""

INDEX_SQL = f"""
CREATE INDEX IF NOT EXISTS idx_{TABLE_TARGET}_search_keywords ON {TABLE_TARGET} USING GIN (search_keywords);

-- ScaNN index placeholder: replace with AlloyDB-specific ScaNN index creation if available.
-- TODO: Replace the next line with your AlloyDB ScaNN CREATE INDEX syntax.
-- Example (pseudo):
-- CREATE INDEX IF NOT EXISTS idx_{TABLE_TARGET}_scann ON {TABLE_TARGET} USING scann (content_embedding);
"""

# ---------------------------
# Embedding ingestion (placeholder)
# ---------------------------
def generate_embedding_demo(text: str) -> List[float]:
    """
    Deterministic demo embedding for offline testing. REPLACE with Vertex Embeddings call.
    Must return a list[float] length 768 (or your model dim) and L2-normalized if using cosine.
    """
    if not text or text.strip() == "":
        return []
    rng = np.random.RandomState(abs(hash(text)) % (2**32))
    v = rng.randn(768).astype(float)
    norm = np.linalg.norm(v)
    if norm > 0:
        v = v / norm
    return v.tolist()

def ingest_embeddings(conn, batch_size=128, dry_run=False):
    """
    Find rows in TABLE_TARGET missing embeddings and generate them.
    Replace generate_embedding_demo with a proper Vertex embedding call in Colab.
    """
    with conn.cursor() as cur:
        cur.execute(f"SELECT id, embedding_text FROM {TABLE_TARGET} WHERE content_embedding IS NULL OR array_length(content_embedding,1) = 0")
        rows = cur.fetchall()
        print("Rows to embed:", len(rows))
        for i in range(0, len(rows), batch_size):
            batch = rows[i:i+batch_size]
            updates = []
            for _id, text in batch:
                if not text or text.strip() == "":
                    emb = []
                else:
                    emb = generate_embedding_demo(text)
                updates.append((_id, emb))
            if not dry_run:
                for _id, emb in updates:
                    cur.execute(
                        f"UPDATE {TABLE_TARGET} SET content_embedding = %s, embedding_model = %s, indexed_at = now() WHERE id = %s",
                        (emb, EMBEDDING_MODEL, _id)
                    )
            print(f"Indexed batch {i} .. {i + len(batch)}")

# ---------------------------
# Paraphrase generation (DSPy-like) placeholder
# ---------------------------
def generate_paraphrases_demo(query_text: str, n: int = 4) -> List[str]:
    """Return 4 candidate rephrasings. Replace with Vertex text generation in Colab for production."""
    base = query_text.strip()
    options = [
        base,
        base + " (refined)",
        "Find pages about: " + base,
        base.rstrip('?') + " - pages"
    ]
    return options[:n]

# ---------------------------
# BM25 & ScaNN SQL templates (placeholders)
# ---------------------------
BM25_SQL_TEMPLATE = f"""
SELECT id, page_url, meta_data, page_content_html, ts_rank(search_keywords, plainto_tsquery(%s)) AS bm25_score
FROM {TABLE_TARGET}
WHERE search_keywords @@ plainto_tsquery(%s)
  AND ts_rank(search_keywords, plainto_tsquery(%s)) >= %s;
"""

SCANN_SQL_TEMPLATE = f"""
-- PSEUDO-SQL: Replace scann_similarity with actual AlloyDB ScaNN function or API call.
SELECT id, page_url, meta_data, page_content_html, scann_similarity(content_embedding, %s) AS vec_sim
FROM {TABLE_TARGET}
WHERE scann_similarity(content_embedding, %s) >= %s;
"""

# ---------------------------
# Normalize, fuse, elbow
# ---------------------------
def normalize_and_fuse(df: pd.DataFrame, alpha=0.4, beta=0.1, gamma=0.05, temp=0.1) -> pd.DataFrame:
    if 'vec_sim' not in df.columns:
        df['vec_sim'] = 0.0
    if 'bm25_score' not in df.columns:
        df['bm25_score'] = 0.0
    df['vec_norm'] = (df['vec_sim'] + 1.0) / 2.0  # map cosine [-1,1] to [0,1]
    bm_mean = df['bm25_score'].mean() if not df['bm25_score'].isnull().all() else 0.0
    bm_std  = df['bm25_score'].std(ddof=0) if df['bm25_score'].std(ddof=0) > 0 else 1.0
    df['bm_z'] = (df['bm25_score'] - bm_mean) / bm_std
    df['bm_norm'] = 1.0 / (1.0 + np.exp(-df['bm_z']))
    df['meta_boost'] = df.get('meta_boost', 0.0)
    df['recency_boost'] = df.get('recency_boost', 0.0)
    df['fused'] = alpha * df['vec_norm'] + (1 - alpha) * df['bm_norm'] + beta * df['meta_boost'] + gamma * df['recency_boost']
    df['fused_softmax'] = softmax(df['fused'] / temp)
    return df

def elbow_cutoff(sorted_scores: List[float], min_keep=MIN_KEEP, max_keep=5000):
    s = np.array(sorted_scores)
    if len(s) <= min_keep:
        return len(s)
    diffs = np.diff(s)
    diffs2 = np.diff(diffs)
    if len(diffs2) == 0:
        return max(min_keep, len(s))
    elbow_idx = np.argmax(diffs2) + 2
    keep = max(min_keep, elbow_idx)
    if keep > max_keep:
        keep = max_keep
    return keep

# ---------------------------
# Cross-encoder NLI placeholder
# ---------------------------
def cross_encoder_nli_demo(pairs: List[Tuple[str,str]]) -> List[Dict[str,float]]:
    """Simulated NLI outputs. Replace with your cross-encoder inference (Vertex or local)."""
    out = []
    for q, d in pairs:
        p_entail = np.random.rand() * 0.8
        p_neutral = np.random.rand() * 0.3
        p_contradict = max(0.0, 1.0 - (p_entail + p_neutral))
        rel_score = np.random.rand()
        out.append({'entail': float(p_entail), 'neutral': float(p_neutral), 'contradict': float(p_contradict), 'rel_score': float(rel_score)})
    return out

# ---------------------------
# Progressive retrieval helper (no top-k semantics)
# ---------------------------
def progressive_union_and_limits(conn, q_text: str, q_vec: List[float], t_vec=T_VEC_INIT, t_bm25=T_BM25_INIT):
    """
    Retrieve candidates by threshold (no top-k). Returns DataFrame of unioned candidates.
    Note: SCANN SQL template is a placeholder. If your AlloyDB doesn't support scann_similarity SQL,
    implement a large top-k ANN call via API and filter by vec_sim >= t_vec here.
    """
    bm25_df = pd.DataFrame(columns=['id','page_url','meta_data','page_content_html','bm25_score'])
    scann_df = pd.DataFrame(columns=['id','page_url','meta_data','page_content_html','vec_sim'])
    # BM25
    try:
        with conn.cursor() as cur:
            cur.execute(BM25_SQL_TEMPLATE, (q_text, q_text, q_text, t_bm25))
            rows = cur.fetchall()
            if rows:
                bm25_df = pd.DataFrame(rows, columns=['id','page_url','meta_data','page_content_html','bm25_score'])
    except Exception as e:
        # BM25 may fail if search_keywords not populated
        print("BM25 SQL failed (demo):", e)
    # ScaNN (placeholder)
    try:
        with conn.cursor() as cur:
            cur.execute(SCANN_SQL_TEMPLATE, (q_vec, q_vec, t_vec))
            rows = cur.fetchall()
            if rows:
                scann_df = pd.DataFrame(rows, columns=['id','page_url','meta_data','page_content_html','vec_sim'])
    except Exception as e:
        # If ScaNN SQL not available, ignore for demo
        #print("ScaNN SQL failed (placeholder):", e)
        scann_df = pd.DataFrame(columns=['id','page_url','meta_data','page_content_html','vec_sim'])

    # Union & dedupe by id
    combined = pd.concat([bm25_df.set_index('id'), scann_df.set_index('id')], axis=1, sort=False).reset_index()
    if combined.shape[0] > 0:
        combined['meta_data'] = combined['meta_data'].apply(lambda x: safe_json_load(x))
        combined['vec_sim'] = combined.get('vec_sim', 0.0)
        combined['bm25_score'] = combined.get('bm25_score', 0.0)
    return combined

# ---------------------------
# End-to-end orchestrator
# ---------------------------
def end_to_end_query(conn, user_query: str, chosen_variant_index: int = 0, show_snippets: bool = True):
    """
    1) Produces 4 paraphrases (DSPy-like)
    2) User selection -> progressive retrieval (no top-k)
    3) Normalize & fuse
    4) NLI cross-encoder gating (simulated) -> demote/keep
    """
    print("User query:", user_query)
    paraphrases = generate_paraphrases_demo(user_query, n=4)
    print("Paraphrase suggestions:")
    for i, p in enumerate(paraphrases):
        print(f"{i+1}. {p}")
    # Choose variant (simulated); in real UI user picks one
    q_text = paraphrases[chosen_variant_index] if 0 <= chosen_variant_index < len(paraphrases) else paraphrases[0]
    q_vec = generate_embedding_demo(q_text)

    # Retrieve candidates by thresholds (no top-k)
    combined = progressive_union_and_limits(conn, q_text, q_vec, t_vec=T_VEC_INIT, t_bm25=T_BM25_INIT)
    print("Initial candidate count (BM25 âˆª ScaNN thresholds):", len(combined))

    # Tighten thresholds progressively if too many candidates
    t_vec = T_VEC_INIT
    t_bm25 = T_BM25_INIT
    while len(combined) > S_SAFE and (t_vec < 0.95 or t_bm25 < 1e6):
        t_vec = min(0.95, t_vec + 0.05)
        t_bm25 = t_bm25 + 0.3
        print(f"Tightening thresholds -> t_vec={t_vec:.3f}, t_bm25={t_bm25:.3f}")
        combined = progressive_union_and_limits(conn, q_text, q_vec, t_vec=t_vec, t_bm25=t_bm25)
        if t_vec >= 0.95 and t_bm25 > 1000:
            break
    if len(combined) > S_HARD:
        print("Candidate set huge; truncating to internal MAX_PROCESS for demo.")
        combined = combined.head(MAX_PROCESS)

    if combined.shape[0] == 0:
        print("No candidates found. Consider relaxing thresholds or indexing more content.")
        return pd.DataFrame()

    # Normalize and fuse
    combined = normalize_and_fuse(combined)
    # NLI candidates: all with fused >= T_FUSE (no top-k)
    nli_candidates = combined[combined['fused'] >= T_FUSE].copy()
    print("NLI candidate count (fused >= T_FUSE):", len(nli_candidates))

    # If none, relax fused threshold
    t_fuse = T_FUSE
    while nli_candidates.shape[0] == 0 and t_fuse > 0.1:
        t_fuse = max(0.1, t_fuse - 0.05)
        nli_candidates = combined[combined['fused'] >= t_fuse].copy()
        print("Relaxed T_FUSE to", t_fuse, "->", len(nli_candidates), "candidates")

    if nli_candidates.shape[0] == 0:
        print("No NLI candidates after relaxing. Returning top fused candidates for inspection.")
        return combined.sort_values('fused', ascending=False).head(MIN_KEEP)

    # Prepare pairs for cross-encoder NLI (batch if implementing real model)
    pairs = []
    for _, row in nli_candidates.iterrows():
        doc_text = (row['page_content_html'] or '')[:2000]
        pairs.append((q_text, doc_text))

    nli_results = cross_encoder_nli_demo(pairs)

    # Apply NLI decisions (demote, keep)
    for i, idx in enumerate(nli_candidates.index):
        res = nli_results[i]
        p_entail, p_neutral, p_contradict = res['entail'], res['neutral'], res['contradict']
        if (p_entail + p_neutral) >= NLI_KEEP:
            decision = 'keep'
        elif p_contradict > NLI_CONTRADICT:
            decision = 'contradiction_demote'
        else:
            decision = 'uncertain_demote'
        nli_candidates.at[idx, 'p_entail'] = p_entail
        nli_candidates.at[idx, 'p_neutral'] = p_neutral
        nli_candidates.at[idx, 'p_contradict'] = p_contradict
        nli_candidates.at[idx, 'cross_rel_score'] = res['rel_score']
        nli_candidates.at[idx, 'nli_decision'] = decision
        final_score = nli_candidates.at[idx, 'fused']
        if decision == 'contradiction_demote':
            final_score *= 0.2
        elif decision == 'uncertain_demote':
            final_score *= 0.6
        nli_candidates.at[idx, 'final_score'] = final_score

    final_df = nli_candidates.sort_values('final_score', ascending=False).reset_index(drop=True)
    print("Final result count:", len(final_df))

    # Prepare display
    display_cols = ['id','page_url','meta_data','bm25_score','vec_sim','fused','p_entail','p_neutral','p_contradict','nli_decision','final_score']
    display_df = final_df[display_cols].copy()
    if show_snippets:
        display_df['snippet'] = final_df['page_content_html'].fillna('').str.slice(0,400)
    pd.set_option('display.max_colwidth', 200)
    if display_df.shape[0] > 0:
        print(display_df.head(50).to_string(index=False))
    return final_df

# ---------------------------
# Quick utilities to run setup & demo
# ---------------------------
def setup_tables_and_indexes(conn):
    """Create target table, populate from source, build search_keywords and create indexes."""
    with conn.cursor() as cur:
        print("Creating target table...")
        cur.execute(CREATE_TABLE_SQL)
        print("Populating target table from source...")
        cur.execute(POPULATE_SQL)
        print("Populating search_keywords tsvector where null...")
        cur.execute(CREATE_SEARCH_KEYWORDS_SQL)
        print("Creating indexes (GIN + ScaNN placeholder)...")
        cur.execute(INDEX_SQL)
    print("Setup complete.")

# ---------------------------
# Usage instructions & example
# ---------------------------
if __name__ == "__main__":
    print("Colab AI Audit pipeline script (demo).")
    print("IMPORTANT: Replace DB_URI and model placeholders before real runs.")
    print("Example quick-run (demo mode):")
    print("1) conn = connect_db()")
    print("2) setup_tables_and_indexes(conn)  # creates table, populates, builds tsvector & index")
    print("3) ingest_embeddings(conn)  # demo embed; replace with Vertex embedding calls")
    print("4) end_to_end_query(conn, 'your query here')")

    # Demo (no DB): show paraphrases example
    print('\\nDemo paraphrases for \"how to improve page SEO\" ->')
    print(generate_paraphrases_demo("how to improve page SEO", n=4))
