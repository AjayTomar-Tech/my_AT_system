# colab_ai_audit_full.py
# Full end-to-end AI Audit pipeline integrated with Google Gemini (Flash + Pro)
# Ready for Colab. Replace placeholders (DB_URI, GCP_PROJECT, GCP_LOCATION, model names).
# Authentication in Colab: use `from google.colab import auth; auth.authenticate_user()` or set ADC.

import os
import json
import time
import math
import hashlib
import pickle
from typing import List, Tuple, Dict, Any
import numpy as np
import pandas as pd
from scipy.special import softmax
import psycopg2
from psycopg2.extras import Json

# Vertex imports (ensure vertexai package installed in Colab)
import vertexai
from vertexai.preview.generative_models import GenerativeModel
from vertexai.preview.language_models import TextEmbeddingModel

# -------------------------
# Configuration placeholders (REPLACE BEFORE RUNNING)
# -------------------------
# Database (AlloyDB) connection: full URI
DB_URI = "postgresql://<USER>:<PASSWORD>@<HOST>:<PORT>/<DB_NAME>"  # <-- paste your AlloyDB URI
TABLE_SOURCE = "site_pages_data"
TABLE_TARGET = "pages_content_audit_2"

# Google Cloud / Vertex settings
GCP_PROJECT = "<YOUR_GCP_PROJECT_ID>"
GCP_LOCATION = "us-central1"

# Models (Gemini & Embeddings)
GEMINI_FLASH = "gemini-1.5-flash"    # fast, low-cost reranker
GEMINI_PRO = "gemini-1.5-pro"        # accurate NLI
EMBEDDING_MODEL_NAME = "textembedding-gecko-001"  # replace with Vertex embedding model name you have

# Retrieval & NLI thresholds (tune on validation)
T_VEC_INIT = 0.30
T_BM25_INIT = 1.5
T_FUSE = 0.60
NLI_KEEP = 0.80
NLI_CONTRADICT = 0.60

# Progressive caps to bound compute (internal)
S_SAFE = 2000
S_HARD = 10000
MIN_KEEP = 10
MAX_PROCESS = 5000

# Gemini batching knobs
FLASH_BATCH_SIZE = 8
PRO_BATCH_SIZE = 4
BATCH_SLEEP_SEC = 0.05

# Cache directory (Colab ephemeral storage)
CACHE_DIR = "/mnt/data/gemini_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

# -------------------------
# DB utilities
# -------------------------
def connect_db():
    """Connect to AlloyDB/Postgres using DB_URI."""
    conn = psycopg2.connect(DB_URI)
    conn.autocommit = True
    return conn

def safe_json_load(s):
    if s is None:
        return {}
    try:
        return json.loads(s) if isinstance(s, str) else s
    except Exception:
        return {}

# -------------------------
# SQL: table creation & populate
# -------------------------
CREATE_TABLE_SQL = f"""
CREATE TABLE IF NOT EXISTS {TABLE_TARGET} (
  id BIGINT PRIMARY KEY,
  original_site_id TEXT,
  page_url TEXT,
  meta_data JSONB,
  search_keywords TSVECTOR,
  page_content_html TEXT,
  embedding_text TEXT,
  content_embedding FLOAT8[],
  embedding_model TEXT,
  quality_score FLOAT8 DEFAULT 0.0,
  indexed_at TIMESTAMP DEFAULT now()
);
"""

POPULATE_SQL = f"""
INSERT INTO {TABLE_TARGET} (id, original_site_id, page_url, meta_data, page_content_html, embedding_text)
SELECT id,
       COALESCE(original_site_id, '') AS original_site_id,
       COALESCE(page_url, '') AS page_url,
       COALESCE(meta_data, '{{}}'::jsonb) AS meta_data,
       COALESCE(page_content_html, '') AS page_content_html,
       COALESCE(
         nullif(trim(both from page_content_html), ''),
         (COALESCE((meta_data->>'title'), '') || ' ' || COALESCE((meta_data->>'description'), '')),
         ''
       ) AS embedding_text
FROM {TABLE_SOURCE}
ON CONFLICT (id) DO NOTHING;
"""

CREATE_SEARCH_KEYWORDS_SQL = f"""
UPDATE {TABLE_TARGET}
SET search_keywords = to_tsvector('english',
    COALESCE(embedding_text,'') || ' ' ||
    COALESCE((meta_data->>'title'), '') || ' ' ||
    COALESCE((meta_data->>'description'), '')
)
WHERE (search_keywords IS NULL);
"""

INDEX_SQL = f"""
CREATE INDEX IF NOT EXISTS idx_{TABLE_TARGET}_search_keywords ON {TABLE_TARGET} USING GIN (search_keywords);

-- TODO: Create ScaNN index according to your AlloyDB ScaNN syntax.
-- Example (pseudo): CREATE INDEX idx_{TABLE_TARGET}_scann ON {TABLE_TARGET} USING scann (content_embedding);
"""

# -------------------------
# Vertex init & embeddings
# -------------------------
def init_vertex(project: str = GCP_PROJECT, location: str = GCP_LOCATION):
    """Initialize Vertex SDK (requires ADC or gcloud auth)."""
    vertexai.init(project=project, location=location)
    print(f"Vertex initialized: project={project}, location={location}")

def generate_embedding_vertex(text: str, model_name: str = EMBEDDING_MODEL_NAME) -> List[float]:
    """Generate a normalized embedding using Vertex TextEmbeddingModel."""
    if not text or text.strip() == "":
        return []
    model = TextEmbeddingModel.from_pretrained(model_name)
    response = model.get_embeddings([text])
    vec = response[0].values
    arr = np.array(vec, dtype=float)
    norm = np.linalg.norm(arr)
    if norm > 0:
        arr = arr / norm
    return arr.tolist()

def generate_embedding_demo(text: str) -> List[float]:
    """Deterministic demo embedding (fallback)."""
    if not text or text.strip() == "":
        return []
    rng = np.random.RandomState(abs(hash(text)) % (2**32))
    v = rng.randn(768).astype(float)
    norm = np.linalg.norm(v)
    if norm > 0:
        v = v / norm
    return v.tolist()

def ingest_embeddings(conn, batch_size: int = 64, use_vertex: bool = False, dry_run: bool = False):
    """Populate content_embedding for rows missing it."""
    with conn.cursor() as cur:
        cur.execute(f"SELECT id, embedding_text FROM {TABLE_TARGET} WHERE content_embedding IS NULL OR array_length(content_embedding,1) = 0")
        rows = cur.fetchall()
        print("Rows to embed:", len(rows))
        for i in range(0, len(rows), batch_size):
            batch = rows[i:i+batch_size]
            updates = []
            for _id, text in batch:
                if not text or text.strip() == "":
                    emb = []
                else:
                    try:
                        emb = generate_embedding_vertex(text) if use_vertex else generate_embedding_demo(text)
                    except Exception as e:
                        print("Embedding generation error:", e)
                        emb = generate_embedding_demo(text)
                updates.append((_id, emb))
            if not dry_run:
                for _id, emb in updates:
                    cur.execute(f"UPDATE {TABLE_TARGET} SET content_embedding = %s, embedding_model = %s, indexed_at = now() WHERE id = %s",
                                (emb, EMBEDDING_MODEL_NAME if use_vertex else "demo", _id))
            print(f"Updated embeddings batch {i}..{i+len(batch)}")

# -------------------------
# Token estimation, chunking, summarization & caching
# -------------------------
def estimate_tokens(text: str) -> int:
    """Rough token estimate (1 token ~ 4 chars)."""
    if not text:
        return 0
    return max(1, int(len(text) / 4))

def chunk_document(doc_text: str, max_tokens_per_chunk: int = 1500) -> List[str]:
    """Split document into token-bounded chunks (attempts paragraph boundaries)."""
    if not doc_text:
        return []
    paragraphs = [p.strip() for p in doc_text.split('\n\n') if p.strip()]
    chunks = []
    current = ""
    current_tokens = 0
    for p in paragraphs:
        p_tokens = estimate_tokens(p)
        if current_tokens + p_tokens <= max_tokens_per_chunk:
            current += ("\n\n" + p) if current else p
            current_tokens += p_tokens
        else:
            if current:
                chunks.append(current)
            if p_tokens > max_tokens_per_chunk:
                approx_chars = max_tokens_per_chunk * 4
                for i in range(0, len(p), approx_chars):
                    chunks.append(p[i:i+approx_chars])
                current = ""
                current_tokens = 0
            else:
                current = p
                current_tokens = p_tokens
    if current:
        chunks.append(current)
    return chunks

def cache_get(key: str):
    path = os.path.join(CACHE_DIR, f"{hashlib.sha256(key.encode()).hexdigest()}.pkl")
    if os.path.exists(path):
        try:
            with open(path, "rb") as f:
                return pickle.load(f)
        except Exception:
            return None
    return None

def cache_put(key: str, value, ttl_seconds: int = 86400):
    path = os.path.join(CACHE_DIR, f"{hashlib.sha256(key.encode()).hexdigest()}.pkl")
    try:
        with open(path, "wb") as f:
            pickle.dump(value, f)
    except Exception:
        pass

def summarize_chunks_with_gemini(chunks: List[str], model_name: str = GEMINI_FLASH, max_out_tokens: int = 256) -> str:
    """Summarize chunks using Gemini Flash and return a short summary. Result is cached."""
    if not chunks:
        return ""
    key = "summ:" + "|".join([str(len(c)) + ":" + hashlib.sha1(c.encode()).hexdigest() for c in chunks])
    cached = cache_get(key)
    if cached:
        return cached
    model = GenerativeModel(model_name)
    prompt = "Summarize the following document chunks into a concise factual summary (2-4 short paragraphs). Output JSON: {\"summary\":\"<text>\"}\n\n"
    for i, c in enumerate(chunks):
        prompt += f"--- CHUNK {i+1} ---\n{c}\n\n"
    try:
        resp = model.generate(prompt, max_output_tokens=max_out_tokens, temperature=0.0)
        txt = resp.text.strip()
        try:
            parsed = json.loads(txt)
            summary = parsed.get("summary", txt)
        except Exception:
            summary = txt
    except Exception as e:
        print("Gemini summarization error:", e)
        summary = chunks[0][:max_out_tokens * 4]
    cache_put(key, summary)
    time.sleep(BATCH_SLEEP_SEC)
    return summary

def prepare_document_text_for_pair(full_doc_text: str, summary_threshold_tokens: int = 1500) -> str:
    """Return a compact representation of a doc: original if short, else summarized chunks."""
    if not full_doc_text:
        return ""
    if estimate_tokens(full_doc_text) <= summary_threshold_tokens:
        return full_doc_text
    chunks = chunk_document(full_doc_text, max_tokens_per_chunk=summary_threshold_tokens)
    summary = summarize_chunks_with_gemini(chunks, model_name=GEMINI_FLASH, max_out_tokens=200)
    if estimate_tokens(summary) <= summary_threshold_tokens:
        return summary
    approx_chars = summary_threshold_tokens * 4
    return summary[:approx_chars]

# -------------------------
# Gemini pairwise scoring & NLI
# -------------------------
def gemini_batch_relevance_score(pairs: List[Tuple[str,str]],
                                 model_name: str = GEMINI_FLASH,
                                 max_output_tokens: int = 32,
                                 batch_size: int = FLASH_BATCH_SIZE) -> List[float]:
    """Batch-call Gemini Flash to get relevance scores in [0,1] for pairs."""
    if not pairs:
        return []
    model = GenerativeModel(model_name)
    scores = []
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i:i+batch_size]
        prompt = "You are a relevance scorer. For each QUERY and DOCUMENT pair, on its own line output JSON {\"relevance\": <float 0.0-1.0>}.\n"
        prompt += "Higher number means more relevant. Be concise and factual.\n\n"
        for idx, (q, d) in enumerate(batch):
            snippet = d if len(d) < 4000 else d[:4000]
            prompt += f"=== PAIR {idx+1} ===\nQUERY:\n{q}\n\nDOCUMENT:\n{snippet}\n\n"
        try:
            resp = model.generate(prompt, max_output_tokens=max_output_tokens, temperature=0.0)
            out_text = resp.text.strip()
            for line in out_text.splitlines():
                line = line.strip()
                if not line:
                    continue
                try:
                    parsed = json.loads(line)
                    val = parsed.get("relevance", None)
                    scores.append(float(val) if val is not None else 0.0)
                except Exception:
                    import re
                    m = re.search(r"([0-9]*\.?[0-9]+)", line)
                    scores.append(float(m.group(1)) if m else 0.0)
        except Exception as e:
            print("Gemini flash relevance error:", e)
            scores.extend([0.0] * len(batch))
        time.sleep(BATCH_SLEEP_SEC)
    return scores

def gemini_nli_pairs(pairs: List[Tuple[str,str]],
                     model_name: str = GEMINI_PRO,
                     max_output_tokens: int = 128,
                     batch_size: int = PRO_BATCH_SIZE) -> List[Dict[str,float]]:
    """Batch-call Gemini Pro to get NLI probabilities (entailment, neutral, contradiction)."""
    if not pairs:
        return []
    model = GenerativeModel(model_name)
    results = []
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i:i+batch_size]
        prompt = ("You are a precise NLI classifier. For each item, output one JSON per line with keys:\n"
                  "{\"entailment\":<0-1>,\"neutral\":<0-1>,\"contradiction\":<0-1>,\"explanation\":\"<short>\"}\n")
        for idx, (q, d) in enumerate(batch):
            snippet = d if len(d) < 4000 else d[:4000]
            prompt += f"--- ITEM {idx+1} ---\nQUERY:\n{q}\n\nDOCUMENT:\n{snippet}\n\n"
        try:
            resp = model.generate(prompt, max_output_tokens=max_output_tokens, temperature=0.0)
            out_text = resp.text.strip()
            for line in out_text.splitlines():
                line = line.strip()
                if not line:
                    continue
                try:
                    parsed = json.loads(line)
                    e = float(parsed.get("entailment", 0.0))
                    n = float(parsed.get("neutral", 0.0))
                    c = float(parsed.get("contradiction", 0.0))
                    expl = parsed.get("explanation", "")
                    s = e + n + c
                    if s <= 0:
                        e, n, c = 0.0, 0.0, 1.0
                    else:
                        e, n, c = e/s, n/s, c/s
                    results.append({"entailment": e, "neutral": n, "contradiction": c, "explanation": expl})
                except Exception:
                    results.append({"entailment":0.0,"neutral":1.0,"contradiction":0.0,"explanation":line})
        except Exception as e:
            print("Gemini pro NLI error:", e)
            for _ in batch:
                results.append({"entailment":0.0,"neutral":1.0,"contradiction":0.0,"explanation":"nli_error"})
        time.sleep(BATCH_SLEEP_SEC)
    return results

# -------------------------
# Retrieval: BM25 + ScaNN thresholded union
# -------------------------
BM25_SQL_TEMPLATE = f"""
SELECT id, page_url, meta_data, page_content_html, ts_rank(search_keywords, plainto_tsquery(%s)) AS bm25_score
FROM {TABLE_TARGET}
WHERE search_keywords @@ plainto_tsquery(%s)
  AND ts_rank(search_keywords, plainto_tsquery(%s)) >= %s;
"""

SCANN_SQL_TEMPLATE = f"""
-- PSEUDO-SQL: Replace scann_similarity with your AlloyDB ScaNN SQL function or use your vector API
SELECT id, page_url, meta_data, page_content_html, scann_similarity(content_embedding, %s) AS vec_sim
FROM {TABLE_TARGET}
WHERE scann_similarity(content_embedding, %s) >= %s;
"""

def progressive_union_and_limits(conn, q_text: str, q_vec: List[float], t_vec: float = T_VEC_INIT, t_bm25: float = T_BM25_INIT):
    """Retrieve BM25 hits above threshold and ScaNN hits above vec threshold, union and dedupe."""
    bm25_df = pd.DataFrame(columns=['id','page_url','meta_data','page_content_html','bm25_score'])
    scann_df = pd.DataFrame(columns=['id','page_url','meta_data','page_content_html','vec_sim'])
    # BM25
    try:
        with conn.cursor() as cur:
            cur.execute(BM25_SQL_TEMPLATE, (q_text, q_text, q_text, t_bm25))
            rows = cur.fetchall()
            if rows:
                bm25_df = pd.DataFrame(rows, columns=['id','page_url','meta_data','page_content_html','bm25_score'])
    except Exception as e:
        print("BM25 retrieval error:", e)
    # ScaNN (placeholder)
    try:
        with conn.cursor() as cur:
            cur.execute(SCANN_SQL_TEMPLATE, (q_vec, q_vec, t_vec))
            rows = cur.fetchall()
            if rows:
                scann_df = pd.DataFrame(rows, columns=['id','page_url','meta_data','page_content_html','vec_sim'])
    except Exception as e:
        # If ScaNN SQL not available, you'll need to call your vector API and filter by vec_sim >= t_vec
        #print("ScaNN retrieval error (placeholder):", e)
        scann_df = pd.DataFrame(columns=['id','page_url','meta_data','page_content_html','vec_sim'])
    combined = pd.concat([bm25_df.set_index('id'), scann_df.set_index('id')], axis=1, sort=False).reset_index()
    if combined.shape[0] > 0:
        combined['meta_data'] = combined['meta_data'].apply(lambda x: safe_json_load(x))
        combined['vec_sim'] = combined.get('vec_sim', 0.0)
        combined['bm25_score'] = combined.get('bm25_score', 0.0)
    return combined

# -------------------------
# Normalize, fuse, NLI gating & final ranking
# -------------------------
def normalize_and_fuse(df: pd.DataFrame, alpha=0.35, beta=0.35, gamma=0.2, delta=0.1, temp=0.1) -> pd.DataFrame:
    """Normalize BM25 & vector scores, incorporate gemini_relevance and produce fused score."""
    if 'vec_sim' not in df.columns:
        df['vec_sim'] = 0.0
    if 'bm25_score' not in df.columns:
        df['bm25_score'] = 0.0
    df['vec_norm'] = (df['vec_sim'] + 1.0) / 2.0
    bm_mean = df['bm25_score'].mean() if not df['bm25_score'].isnull().all() else 0.0
    bm_std  = df['bm25_score'].std(ddof=0) if df['bm25_score'].std(ddof=0) > 0 else 1.0
    df['bm_z'] = (df['bm25_score'] - bm_mean) / bm_std
    df['bm_norm'] = 1.0 / (1.0 + np.exp(-df['bm_z']))
    df['gemini_relevance'] = df.get('gemini_relevance', 0.0)
    df['meta_boost'] = df.get('meta_boost', 0.0)
    df['fused'] = alpha * df['vec_norm'] + beta * df['bm_norm'] + gamma * df['gemini_relevance'] + delta * df['meta_boost']
    df['fused_softmax'] = softmax(df['fused'] / temp)
    return df

def apply_nli_and_decisions(df: pd.DataFrame, query_text: str) -> pd.DataFrame:
    """Run Gemini Pro NLI on candidates and apply demote/keep decisions producing final_score."""
    nli_df = df.copy()
    pairs = []
    idxs = []
    for idx, row in nli_df.iterrows():
        doc_text = row.get('page_content_html') or ""
        small_doc = prepare_document_text_for_pair(doc_text, summary_threshold_tokens=2000)
        pairs.append((query_text, small_doc))
        idxs.append(idx)
    if not pairs:
        return df
    nli_results = gemini_nli_pairs(pairs, model_name=GEMINI_PRO, batch_size=PRO_BATCH_SIZE)
    for idx, res in zip(idxs, nli_results):
        e, n, c = res['entailment'], res['neutral'], res['contradiction']
        nli_df.at[idx, 'p_entail'] = e
        nli_df.at[idx, 'p_neutral'] = n
        nli_df.at[idx, 'p_contradict'] = c
        nli_df.at[idx, 'nli_explanation'] = res.get('explanation', '')
        if (e + n) >= NLI_KEEP:
            decision = 'keep'
        elif c > NLI_CONTRADICT:
            decision = 'contradiction_demote'
        else:
            decision = 'uncertain_demote'
        nli_df.at[idx, 'nli_decision'] = decision
        final_score = nli_df.at[idx, 'fused']
        if decision == 'contradiction_demote':
            final_score *= 0.2
        elif decision == 'uncertain_demote':
            final_score *= 0.6
        nli_df.at[idx, 'final_score'] = final_score
    return nli_df

# -------------------------
# Full orchestrator: end-to-end query
# -------------------------
def end_to_end_query(conn, user_query: str, chosen_variant_index: int = 0, use_vertex_embeddings: bool = False):
    """
    End-to-end flow:
    - generate paraphrases (4) and pick one
    - thresholded BM25 + ScaNN union (no top-k)
    - prepare doc summaries & Gemini Flash rerank
    - fuse signals & pick NLI candidates (fused >= T_FUSE)
    - Gemini Pro NLI gating & final scoring
    """
    print("User query:", user_query)
    # Paraphrases (DSPy-like)
    paraphrases = [
        user_query.strip(),
        user_query.strip() + " (refined)",
        "Find pages about: " + user_query.strip(),
        user_query.strip().rstrip('?') + " - pages"
    ]
    print("Paraphrase suggestions:")
    for i, p in enumerate(paraphrases):
        print(f"{i+1}. {p}")
    q_text = paraphrases[chosen_variant_index] if 0 <= chosen_variant_index < len(paraphrases) else paraphrases[0]

    # Query embedding (optional Vertex)
    if use_vertex_embeddings:
        try:
            q_vec = generate_embedding_vertex(q_text)
        except Exception as e:
            print("Vertex embedding failed:", e)
            q_vec = generate_embedding_demo(q_text)
    else:
        q_vec = generate_embedding_demo(q_text)

    # Threshold retrieval (no top-k)
    combined = progressive_union_and_limits(conn, q_text, q_vec, t_vec=T_VEC_INIT, t_bm25=T_BM25_INIT)
    print("Initial candidate count (BM25 âˆª ScaNN thresholds):", len(combined))

    # Progressive tightening if candidate explosion
    t_vec = T_VEC_INIT
    t_bm25 = T_BM25_INIT
    while len(combined) > S_SAFE and (t_vec < 0.95 or t_bm25 < 1e6):
        t_vec = min(0.95, t_vec + 0.05)
        t_bm25 = t_bm25 + 0.3
        print(f"Tightening thresholds -> t_vec={t_vec:.3f}, t_bm25={t_bm25:.3f}")
        combined = progressive_union_and_limits(conn, q_text, q_vec, t_vec=t_vec, t_bm25=t_bm25)
        if t_vec >= 0.95 and t_bm25 > 1000:
            break
    if len(combined) > S_HARD:
        print("Candidate set huge; truncating to internal MAX_PROCESS for demo.")
        combined = combined.head(MAX_PROCESS)

    if combined.shape[0] == 0:
        print("No candidates found.")
        return pd.DataFrame()

    # Prepare doc summaries and Gem Flash rerank
    pairs_for_rerank = []
    idxs = []
    for idx, row in combined.iterrows():
        doc_text = row.get('page_content_html') or ""
        cache_key = "summ_doc_" + str(row['id'])
        summary = cache_get(cache_key)
        if summary is None:
            summary = prepare_document_text_for_pair(doc_text, summary_threshold_tokens=1200)
            cache_put(cache_key, summary)
        pairs_for_rerank.append((q_text, summary))
        idxs.append(idx)
    rel_scores = gemini_batch_relevance_score(pairs_for_rerank, model_name=GEMINI_FLASH, batch_size=FLASH_BATCH_SIZE)
    combined['gemini_relevance'] = 0.0
    for idx, s in zip(idxs, rel_scores):
        combined.at[idx, 'gemini_relevance'] = s

    # Normalize & fuse
    combined = normalize_and_fuse(combined)

    # NLI candidates (fused threshold)
    nli_candidates = combined[combined['fused'] >= T_FUSE].copy()
    print("NLI candidate count (fused >= T_FUSE):", len(nli_candidates))
    t_fuse = T_FUSE
    while nli_candidates.shape[0] == 0 and t_fuse > 0.1:
        t_fuse = max(0.1, t_fuse - 0.05)
        nli_candidates = combined[combined['fused'] >= t_fuse].copy()
        print("Relaxed T_FUSE to", t_fuse, "->", len(nli_candidates))

    if nli_candidates.shape[0] == 0:
        print("No NLI candidates after relaxing. Returning top fused candidates.")
        return combined.sort_values('fused', ascending=False).head(MIN_KEEP)

    # Apply Gemini Pro NLI gating and final ranking
    final_df = apply_nli_and_decisions(nli_candidates, q_text)
    final_df = final_df.sort_values('final_score', ascending=False).reset_index(drop=True)
    print("Final results count:", len(final_df))

    display_cols = ['id','page_url','meta_data','bm25_score','vec_sim','gemini_relevance','fused','p_entail','p_neutral','p_contradict','nli_decision','final_score']
    display_df = final_df[display_cols].copy()
    display_df['snippet'] = final_df['page_content_html'].fillna('').str.slice(0,400)
    pd.set_option('display.max_colwidth', 200)
    if display_df.shape[0] > 0:
        print(display_df.head(50).to_string(index=False))
    return final_df

# -------------------------
# Paraphrase & demo helper (kept here for convenience)
# -------------------------
def generate_paraphrases_demo(query_text: str, n: int = 4) -> List[str]:
    base = query_text.strip()
    opts = [
        base,
        base + " (refined)",
        "Find pages about: " + base,
        base.rstrip('?') + " - pages"
    ]
    return opts[:n]

# -------------------------
# Setup helper: create/populate/index
# -------------------------
def setup_tables_and_indexes(conn):
    with conn.cursor() as cur:
        print("Creating target table...")
        cur.execute(CREATE_TABLE_SQL)
        print("Populating target table from source...")
        cur.execute(POPULATE_SQL)
        print("Populating search_keywords tsvector where null...")
        cur.execute(CREATE_SEARCH_KEYWORDS_SQL)
        print("Creating indexes (GIN + ScaNN placeholder)...")
        cur.execute(INDEX_SQL)
    print("Setup complete.")

# -------------------------
# End of module
# -------------------------
if __name__ == "__main__":
    print("colab_ai_audit_full module loaded. Import and call functions interactively in Colab.")
    print("Typical flow:\n  conn = connect_db()\n  setup_tables_and_indexes(conn)\n  ingest_embeddings(conn, use_vertex=True)\n  init_vertex(GCP_PROJECT, GCP_LOCATION)\n  results = end_to_end_query(conn, 'your query here', use_vertex_embeddings=True)")
