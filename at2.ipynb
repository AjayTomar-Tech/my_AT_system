# full_colab_gemini_ai_audit.py
# End-to-end AI Audit pipeline integrated with Google Gemini (Flash + Pro)
# Run in Google Colab. Replace placeholders (DB_URI, GCP_PROJECT, GCP_LOCATION, model names).
# Authentication: use `gcloud auth login` or Colab ADC (service account) before calling Vertex.

# 0) (In Colab) Install required libraries (run in its own cell)
# !pip install --upgrade google-cloud-aiplatform vertexai psycopg2-binary pandas numpy scipy sentencepiece

# -------------------------
# Imports
# -------------------------
import os
import json
import time
import math
import hashlib
import pickle
from typing import List, Tuple, Dict, Any
import numpy as np
import pandas as pd
from scipy.special import softmax
import psycopg2
from psycopg2.extras import Json

# Vertex (Gemini) imports
# Depending on the vertexai version you have, submodule names may vary slightly.
# This code assumes the 'vertexai' package / preview generative models API.
import vertexai
from vertexai.preview.generative_models import GenerativeModel
from vertexai.preview.language_models import TextEmbeddingModel  # for embeddings

# -------------------------
# User configuration - REPLACE THESE
# -------------------------
DB_URI = "postgresql://<USER>:<PASSWORD>@<HOST>:<PORT>/<DB_NAME>"  # <-- paste your AlloyDB connection URI
TABLE_SOURCE = "site_pages_data"
TABLE_TARGET = "pages_content_audit_2"

GCP_PROJECT = "<YOUR_GCP_PROJECT_ID>"
GCP_LOCATION = "us-central1"    # change if needed

# Gemini model names (Vertex)
GEMINI_FLASH = "gemini-1.5-flash"  # fast / low-cost
GEMINI_PRO = "gemini-1.5-pro"      # high-accuracy NLI
EMBEDDING_MODEL_NAME = "textembedding-gecko-001"  # replace with actual Vertex embedding name if different

# Pipeline thresholds (tune on validation)
T_VEC_INIT = 0.30
T_BM25_INIT = 1.5
T_FUSE = 0.60
NLI_KEEP = 0.80
NLI_CONTRADICT = 0.60

# Progressive caps
S_SAFE = 2000
S_HARD = 10000
MIN_KEEP = 10
MAX_PROCESS = 5000

# Gemini batching knobs
FLASH_BATCH_SIZE = 8
PRO_BATCH_SIZE = 4
BATCH_SLEEP_SEC = 0.05

# Local cache paths (Colab filesystem); change if you prefer GCS
CACHE_DIR = "/mnt/data/gemini_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

# -------------------------
# DB connection helpers
# -------------------------
def connect_db():
    conn = psycopg2.connect(DB_URI)
    conn.autocommit = True
    return conn

def safe_json_load(s):
    if s is None:
        return {}
    try:
        return json.loads(s) if isinstance(s, str) else s
    except Exception:
        return {}

# -------------------------
# SQL: table creation & populate
# -------------------------
CREATE_TABLE_SQL = f"""
CREATE TABLE IF NOT EXISTS {TABLE_TARGET} (
  id BIGINT PRIMARY KEY,
  original_site_id TEXT,
  page_url TEXT,
  meta_data JSONB,
  search_keywords TSVECTOR,
  page_content_html TEXT,
  embedding_text TEXT,
  content_embedding FLOAT8[],
  embedding_model TEXT,
  quality_score FLOAT8 DEFAULT 0.0,
  indexed_at TIMESTAMP DEFAULT now()
);
"""

POPULATE_SQL = f"""
INSERT INTO {TABLE_TARGET} (id, original_site_id, page_url, meta_data, page_content_html, embedding_text)
SELECT id,
       COALESCE(original_site_id, '') AS original_site_id,
       COALESCE(page_url, '') AS page_url,
       COALESCE(meta_data, '{{}}'::jsonb) AS meta_data,
       COALESCE(page_content_html, '') AS page_content_html,
       COALESCE(
         nullif(trim(both from page_content_html), ''),
         (COALESCE((meta_data->>'title'), '') || ' ' || COALESCE((meta_data->>'description'), '')),
         ''
       ) AS embedding_text
FROM {TABLE_SOURCE}
ON CONFLICT (id) DO NOTHING;
"""

CREATE_SEARCH_KEYWORDS_SQL = f"""
UPDATE {TABLE_TARGET}
SET search_keywords = to_tsvector('english',
    COALESCE(embedding_text,'') || ' ' ||
    COALESCE((meta_data->>'title'), '') || ' ' ||
    COALESCE((meta_data->>'description'), '')
)
WHERE (search_keywords IS NULL);
"""

INDEX_SQL = f"""
CREATE INDEX IF NOT EXISTS idx_{TABLE_TARGET}_search_keywords ON {TABLE_TARGET} USING GIN (search_keywords);
-- TODO: Add AlloyDB ScaNN index creation according to your instance syntax.
-- Example pseudo-line (replace with your DB's syntax):
-- CREATE INDEX IF NOT EXISTS idx_{TABLE_TARGET}_scann ON {TABLE_TARGET} USING scann (content_embedding);
"""

# -------------------------
# Embedding ingestion (Vertex supported)
# -------------------------
def init_vertex(project: str = GCP_PROJECT, location: str = GCP_LOCATION):
    vertexai.init(project=project, location=location)
    print(f"Vertex initialized for project={project}, location={location}")

def generate_embedding_vertex(text: str, model_name: str = EMBEDDING_MODEL_NAME) -> List[float]:
    """
    Use Vertex TextEmbeddingModel to generate embeddings.
    In Colab, ensure you have authenticated (gcloud auth login or ADC).
    """
    if not text or text.strip() == "":
        return []
    model = TextEmbeddingModel.from_pretrained(model_name)
    response = model.get_embeddings([text])
    vec = response[0].values
    # L2 normalize
    arr = np.array(vec, dtype=float)
    norm = np.linalg.norm(arr)
    if norm > 0:
        arr = arr / norm
    return arr.tolist()

def generate_embedding_demo(text: str) -> List[float]:
    """Deterministic fallback generator for testing offline or if embeddings are unavailable."""
    if not text or text.strip() == "":
        return []
    rng = np.random.RandomState(abs(hash(text)) % (2**32))
    v = rng.randn(768).astype(float)
    norm = np.linalg.norm(v)
    if norm > 0:
        v = v / norm
    return v.tolist()

def ingest_embeddings(conn, batch_size=64, use_vertex=False, dry_run=False):
    """
    Populate content_embedding for rows missing it.
    If use_vertex=True, calls Vertex embedding model (fast but requires auth).
    """
    with conn.cursor() as cur:
        cur.execute(f"SELECT id, embedding_text FROM {TABLE_TARGET} WHERE content_embedding IS NULL OR array_length(content_embedding,1) = 0")
        rows = cur.fetchall()
        print("Rows to embed:", len(rows))
        for i in range(0, len(rows), batch_size):
            batch = rows[i:i+batch_size]
            updates = []
            for _id, text in batch:
                if not text or text.strip() == "":
                    emb = []
                else:
                    try:
                        emb = generate_embedding_vertex(text) if use_vertex else generate_embedding_demo(text)
                    except Exception as e:
                        print("Embedding generation error:", e)
                        emb = generate_embedding_demo(text)
                updates.append((_id, emb))
            if not dry_run:
                for _id, emb in updates:
                    cur.execute(f"UPDATE {TABLE_TARGET} SET content_embedding = %s, embedding_model = %s, indexed_at = now() WHERE id = %s",
                                (emb, EMBEDDING_MODEL_NAME if use_vertex else "demo", _id))
            print(f"Updated embeddings batch {i}..{i+len(batch)}")

# -------------------------
# Token estimation, chunking, summarization (to guarantee token safety)
# -------------------------
def estimate_tokens(text: str) -> int:
    if not text:
        return 0
    # Rough heuristic: 4 characters ~= 1 token
    return max(1, int(len(text) / 4))

def chunk_document(doc_text: str, max_tokens_per_chunk: int = 1500) -> List[str]:
    if not doc_text:
        return []
    paragraphs = [p.strip() for p in doc_text.split('\n\n') if p.strip()]
    chunks = []
    current = ""
    current_tokens = 0
    for p in paragraphs:
        p_tokens = estimate_tokens(p)
        if current_tokens + p_tokens <= max_tokens_per_chunk:
            current += ("\n\n" + p) if current else p
            current_tokens += p_tokens
        else:
            if current:
                chunks.append(current)
            if p_tokens > max_tokens_per_chunk:
                approx_chars = max_tokens_per_chunk * 4
                for i in range(0, len(p), approx_chars):
                    chunks.append(p[i:i+approx_chars])
                current = ""
                current_tokens = 0
            else:
                current = p
                current_tokens = p_tokens
    if current:
        chunks.append(current)
    return chunks

def cache_get(key: str):
    path = os.path.join(CACHE_DIR, f"{hashlib.sha256(key.encode()).hexdigest()}.pkl")
    if os.path.exists(path):
        try:
            with open(path, "rb") as f:
                return pickle.load(f)
        except Exception:
            return None
    return None

def cache_put(key: str, value, ttl_seconds: int = 86400):
    path = os.path.join(CACHE_DIR, f"{hashlib.sha256(key.encode()).hexdigest()}.pkl")
    try:
        with open(path, "wb") as f:
            pickle.dump(value, f)
    except Exception:
        pass

def summarize_chunks_with_gemini(chunks: List[str], model_name: str = GEMINI_FLASH, max_out_tokens: int = 256) -> str:
    """Summarize chunks into concise factual summary using Gemini Flash (cheap)."""
    if not chunks:
        return ""
    key = "summ:" + "|".join([str(len(c)) + ":" + hashlib.sha1(c.encode()).hexdigest() for c in chunks])
    cached = cache_get(key)
    if cached:
        return cached
    model = GenerativeModel(model_name)
    prompt = "Summarize the following document chunks into a concise factual summary (2-4 short paragraphs). Output JSON: {\"summary\":\"<text>\"}\n\n"
    for i, c in enumerate(chunks):
        prompt += f"--- CHUNK {i+1} ---\n{c}\n\n"
    try:
        resp = model.generate(prompt, max_output_tokens=max_out_tokens, temperature=0.0)
        txt = resp.text.strip()
        try:
            parsed = json.loads(txt)
            summary = parsed.get("summary", txt)
        except Exception:
            summary = txt
    except Exception as e:
        print("Gemini summarization error:", e)
        summary = chunks[0][:max_out_tokens * 4]
    cache_put(key, summary)
    time.sleep(BATCH_SLEEP_SEC)
    return summary

def prepare_document_text_for_pair(full_doc_text: str, summary_threshold_tokens: int = 1500) -> str:
    """Return a compact doc text: either original if small or a summary of chunks if large."""
    if not full_doc_text:
        return ""
    if estimate_tokens(full_doc_text) <= summary_threshold_tokens:
        return full_doc_text
    chunks = chunk_document(full_doc_text, max_tokens_per_chunk=summary_threshold_tokens)
    summary = summarize_chunks_with_gemini(chunks, model_name=GEMINI_FLASH, max_out_tokens=200)
    if estimate_tokens(summary) <= summary_threshold_tokens:
        return summary
    approx_chars = summary_threshold_tokens * 4
    return summary[:approx_chars]

# -------------------------
# Gemini pairwise relevance scoring (Flash) and NLI (Pro)
# -------------------------
def gemini_batch_relevance_score(pairs: List[Tuple[str,str]],
                                 model_name: str = GEMINI_FLASH,
                                 max_output_tokens: int = 32,
                                 batch_size: int = FLASH_BATCH_SIZE) -> List[float]:
    if not pairs:
        return []
    model = GenerativeModel(model_name)
    scores = []
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i:i+batch_size]
        prompt = "You are a relevance scorer. For each QUERY and DOCUMENT pair, on its own line output JSON {\"relevance\": <float 0.0-1.0>}.\n"
        prompt += "Higher number means more relevant. Be concise and factual.\n\n"
        for idx, (q, d) in enumerate(batch):
            snippet = d if len(d) < 4000 else d[:4000]
            prompt += f"=== PAIR {idx+1} ===\nQUERY:\n{q}\n\nDOCUMENT:\n{snippet}\n\n"
        try:
            resp = model.generate(prompt, max_output_tokens=max_output_tokens, temperature=0.0)
            out_text = resp.text.strip()
            for line in out_text.splitlines():
                line = line.strip()
                if not line:
                    continue
                try:
                    parsed = json.loads(line)
                    val = parsed.get("relevance", None)
                    scores.append(float(val) if val is not None else 0.0)
                except Exception:
